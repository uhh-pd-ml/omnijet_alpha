_target_: gabbro.models.backbone.BackboneNextTokenPredictionLightning

model_kwargs:
  embedding_dim: 256
  attention_dropout: 0.1
  vocab_size: 8194
  max_sequence_len: 128
  n_GPT_blocks: 3
  n_heads: 8
  verbosity: false

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 0.001 # ParT paper uses RAdam optimizer with initial lr of 0.001
  weight_decay: 0

scheduler:
  _target_: torch.optim.lr_scheduler.ConstantLR
  _partial_: true
